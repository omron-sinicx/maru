theme: dark
organization: OMRON SINIC X
twitter: '@omron_sinicx'
title: 'maru: a miniature-sized wheeled robot for swarm robotics research'
conference: CHI2024
resources:
  paper: https://dl.acm.org/doi/10.1145/3613904.3642870
  arxiv:
  code: https://github.com/omron-sinicx/maru
  video: #
  blog: # https://medium.com/sinicx/our-paper-which-explores-whether-people-can-perceive-as-if-swarm-robots-were-part-of-their-body-69bc10abfd64
description: maru (= miniature assemblage adaptive robot unit) is a custom-made, miniature-sized, two-wheeled robot designed specifically for tabletop swarm robotics research. This platform enables researchers and hobbyists to explore the dynamics of swarm robotics in a controlled, tabletop environment.
image: https://omron-sinicx.github.io/maru/assets/teaser.png
url: https://omron-sinicx.github.io/maru
speakerdeck: #
authors:
  - name: Shigeo Yoshida
    affiliation: [1]
    position: Principal Investigator
    url: https://shigeodayo.me
  - name: Takefumi Hiraki
    affiliation: [2]
    position: Senior Research Scientist
    url: https://takefumihiraki.com/
  - name: Reo Matsumura
    affiliation: [3]
    url: yhttps://krkrpro.com/contact
    position: Engineer
contact_ids: ['karakuri', 1] #=> github issues, contact@sinicx.com, 2nd author
affiliations:
  - name: OMRON SINIC X Corporation
    url: https://www.omron.com/sinicx/
    logo: sinicx.svg
  - name: Cluster Metaverse Lab
    url: https://lab.cluster.mu/
    logo: clusterlogo_1line_solid_color_dark.svg
  - name: Karakuri Products, Inc.
    url: https://krkrpro.com/
    logo: karakuri_products_LOGO_white.svg
meta:
  # none
bibtex: >
  @inproceedings{ichihashi2024swarm,
    author = {Ichihashi, Sosuke and Kuroki, So and Nishimura, Mai and Kasaura, Kazumi and Hiraki, Takefumi and Tanaka, Kazutoshi and Yoshida, Shigeo},
    title = {Swarm Body: Embodied Swarm Robots},
    year = {2024},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {[https://doi.org/10.1145/3613904.3642870](https://doi.org/10.1145/3613904.3642870)},
    doi = {10.1145/3613904.3642870},
    booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
    numpages = {19},
    location = {, Honolulu, HI, USA, },
    series = {CHI '24}
  }
header:
  bg_curve: sinic_curve.png
  bg_image: https://github.com/omron-sinicx/maru/raw/main/images/teaser.jpg

teaser: teaser.png
overview: |

slideshow:
  - teaser.png
  - sb_robot.png
  - exploded_real_naname.jpg
  - teaser.jpg
  - maru_cradle.jpg

body:
  - title: maru
  - text: |
      **"maru" (= miniature assemblage adaptive robot unit)** is a custom-made, miniature-sized, two-wheeled robot designed specifically for tabletop swarm robotics research. This platform enables researchers and hobbyists to explore the dynamics of swarm robotics in a controlled, tabletop environment.

      The [GitHub repository](https://github.com/omron-sinicx/maru) serves as a comprehensive resource for assembling and operating maru, providing detailed documentation and files, including:
        - `hardware/3Dmodel` 3D models of the robot and its cradle (a combined communication and charging module),
        - `hardware/circuit/schematic` Circuit schematics to guide the electronic assembly,
        - `hardware/circuit/gerber` Gerber files for PCB manufacturing,
        - `software/firmware` Firmware for both the robot and the cradle, distributed in binary format (Note: There are currently no plans to release the source code),
        - `software/control` APIs and sample codes to help you get started with programming the robot for various tasks and behaviors (C# and Python).

      This project draws inspiration from [Zooids](https://github.com/ShapeLab/SwarmUI), an open-source swarm robot platform. Similar to Zooids, maru employs projection-based position tracking; however both maru's hardware and software are entirely new designs, another platform for swarm robotics research in HCI.
  - title: Hardware
    image: robot_exploded.png
    text: |
      The hardware design is shown above. The dimensions are 30 mm in diameter and 34 mm in height, weighing about 23 g. The robot parts include a microcontroller unit (STM32G071KBU62 from STMicroelectronics), motor drivers (DRV8837DSGR3 from Texas Instruments), RF module (RF2401F204 from NiceRF), motors with a 26:1 planetary gearbox (Pololu 23575), photodiode (PD15-22C/TR86 from Everlight Electronics), and a Li-Po battery (80mAh).

      Since the robot is lightweight, the robot may fall over while moving if the moving speed is too fast. Magnets can be attached to the bottom of the robot, and the robot can be run on steel or other materials to keep it from falling over.

      A detailed description of the hardware design and specifications is available [here](https://shigeodayo.notion.site/Hardware-Manual-maru-8442a3ade0ba457dba1eb5f8b898ae2d).
  - title: Software
    image: robot_communication.png
    text: |
      The communication between the robots and the host computer is shown above. The robots and the cradles (charging station + RF/IO dongle) are each equipped with an RF module and communicate through a 2.4GHz ISM band wireless communication. The user does not need to be aware of the communication control between the robot and the cradles, as it is performed on the firmware of both devices. The user can easily control the robot wirelessly by sending the specified commands.

      A detailed description of the software design and specifications is available [here](https://shigeodayo.notion.site/Software-Manual-maru-53adfd3b5a8c482b9a2acadc83991381).
  - title: Projection-based Localization
    text: |
      The maru system supports two types of projection-based localization methods: the **Zooids-based** method and **Pixel-level Visible Light Communication (PVLC)** [1] based method. Both methods employ a high-speed projector (DLP LightCrafter 4500 from Texas Instruments) to embed position-encoded patterns into projected images. The two photodiodes on each robot receive the projected coded-pattern light, and the robot’s microcontroller decodes these patterns into position information. Subsequently, the robot calculates its orientation from the positions of the two photodiodes and broadcasts its position and orientation information to the host computer. The details of each method are described below.

      [Zooids-based Method]
      This method projects a sequence of gray-coded patterns independently. A single white image is projected, allowing the projector to be used as an illumination source. For details on this localization method and setup instructions, please refer to [Zooids documentation](https://github.com/ShapeLab/SwarmUI/tree/master/Hardware/Projector%20Tracking%20Setup).

      [PVLC-based Method]
      This method embeds a sequence of gray-coded patterns into video content for projection. By properly configuring the data sent to the projector, **visual content can be projected** while maintaining localization capabilities. For detailed information on this localization method, please refer to the paper [2].

      1. Sho Kimura, Ryo Oguchi, Hideo Tanida, Yasuaki Kakehi, Keita Takahashi, and Takeshi Naemura. 2008. PVLC projector: image projection with imperceptible pixel-level metadata. In ACM SIGGRAPH 2008 posters (SIGGRAPH '08). Association for Computing Machinery, New York, NY, USA, Article 135, 1. https://doi.org/10.1145/1400885.1401030

      2. Takefumi Hiraki, Shogo Fukushima, Yoshihiro Kawahara, and Takeshi Naemura, “Phygital Field: An Integrated Field with Physical Robots and Digital Images using Projection-based Localization and Control Method,” SICE Journal of Control, Measurement, and System Integration, vol. 11, no. 4, pp. 302–311, 2018.7. https://doi.org/10.9746/jcmsi.11.302

  - title: maru vs. Other Platforms
    text: |
      We compared *maru* with similar small mobile robot platforms to highlight differences in localization, communication, hardware specifications, and availability.
      <style>
        .table-container {
          overflow-x: auto;
        }
        table.compare {
          border-collapse: collapse;
          width: 100%;
          min-width: 1000px;
          font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
          font-size: 14px;
        }
        table.compare th, table.compare td {
          padding: 10px 12px;
          vertical-align: top;
        }
        /* ヘッダー行の背景色と文字色 */
        table.compare thead th {
          background-color: #f5f5f5;
          font-weight: 600;
          color: #333;
          border-bottom: 2px solid #ccc;
        }
        /* データ行のストライプ */
        table.compare tbody tr:nth-child(even) {
          background-color: #fafafa;
        }
        /* maru 行の強調 */
        .highlight {
          background-color: #fff8dc !important;
        }
      </style>

      <div class="table-container">
        <table class="compare">
          <thead>
            <tr>
              <th>Name</th>
              <th>Position Localization</th>
              <th>Communication</th>
              <th>Dimensions</th>
              <th>Weight</th>
              <th>Speed</th>
              <th>Sensors</th>
              <th>Battery Capacity</th>
              <th>Available for Purchase</th>
              <th>Manufacturing Method Open</th>
            </tr>
          </thead>
          <tbody>
            <tr class="highlight">
              <td><a href="https://github.com/omron-sinicx/swarm-body" target="_blank">maru</a></td>
              <td>Projection (Zooids-based method or PVLC-based method)</td>
              <td>RF (compliant with TELEC or FCC certified)</td>
              <td>30 x 34 mm (Φ x H)</td>
              <td>23 g</td>
              <td>500 mm/s</td>
              <td>6-axis IMU sensor (3-axis accelerometer, 3-axis gyroscope)</td>
              <td>80 mAh</td>
              <td>Yes</td>
              <td>Yes</td>
            </tr>
            <tr>
              <td><a href="https://github.com/ShapeLab/SwarmUI" target="_blank">Zooids</a></td>
              <td>Projection (Zooids-based method only)</td>
              <td>RF</td>
              <td>26 x 21 mm (Φ x H)</td>
              <td>12 g</td>
              <td>500 mm/s</td>
              <td>Touch</td>
              <td>100 mAh</td>
              <td>No</td>
              <td>Yes</td>
            </tr>
            <tr>
              <td><a href="https://github.com/toio" target="_blank">toio</a></td>
              <td>Pattern recognition</td>
              <td>BLE</td>
              <td>32 x 32 x 26 mm (D x W x H)</td>
              <td>25 g</td>
              <td>Linear: 350 mm/s<br>Rotation: 1500 deg/s</td>
              <td>Magnetic sensor, 3-axis accelerometer, 3-axis gyroscope</td>
              <td>260 mAh</td>
              <td>Yes</td>
              <td>No</td>
            </tr>
          </tbody>
        </table>
      </div>

  - title: License
    text: |
      The software materials of this project are under an [MIT license](https://github.com/omron-sinicx/maru/blob/main/LICENSE). <br />
      The hardware materials of this project are under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).

projects:
  - title: 'FluidicSwarm: Embodiment of Swarm Robots Using Fluid Behavior Imitation'
    journal: "SIGGRAPH'25 Emerging Technologies"
    img: featured_fluidicswarm.jpg
    description: |
      This project introduces FluidicSwarm, a control system that enables users to intuitively manipulate robot swarms as fluid-like extensions of their own bodies.
    url: https://mvml.slis.tsukuba.ac.jp/research/fluidicswarm/
  - title: 'Swarm Body: Embodied Swarm Robots'
    journal: "CHI'24"
    img: featured_swarm.png
    description: |
      Embodied swarm robots allow a human to dynamically shape and disassemble their bodies to physically and adaptively interact with the local and remote environments.
    url: https://shigeodayo.me/works/swarm_body/
  - title: 'Multi-Agent Behavior Retrieval: Retrieval-Augmented Policy Training for Cooperative Push Manipulation by Mobile Robots'
    journal: "IROS'24"
    img: 'https://omron-sinicx.github.io/mabr/assets/method.png'
    description: |
      This project introduces the Multi-Agent Coordination Skill Database, allowing multiple mobile robots to efficiently use past memories to adapt to new tasks.
    url: https://omron-sinicx.github.io/mabr/
  - title: 'Language-Guided Pattern Formation for Swarm Robotics with Multi-Agent Reinforcement Learning'
    journal: "IROS'24"
    img: 'https://omron-sinicx.github.io/language-guided-pattern-formation/assets/overview.jpg'
    description: |
      This project explores how to leverage the vast knowledge encoded in large language models to tackle pattern formation challenges for swarm robotics systems.
    url: https://omron-sinicx.github.io/language-guided-pattern-formation/
